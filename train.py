import torch
import json
import os
import logging
from dataclasses import dataclass, field
from typing import Dict, Optional, List, Sequence
from datasets import Dataset
import transformers
from transformers import (
    TrainingArguments,
    Trainer,
    Qwen2_5_VLForConditionalGeneration, 
    AutoProcessor,
    AutoTokenizer
)
from peft import LoraConfig, TaskType, get_peft_model
from qwen_vl_utils import process_vision_info
import swanlab
from swanlab.integration.transformers import SwanLabCallback
from transformers import AutoConfig

# --- 1. é…ç½®åŒºåŸŸ ---
# å»ºè®®ä½¿ç”¨ç»å¯¹è·¯å¾„
MODEL_PATH = "/data/srq/Qwen/Qwen/Qwen2.5-VL-7B-Instruct" 
OUTPUT_DIR = "./output/Qwen2.5-VL-Video-SFT"
MAX_LENGTH = 4096 
FREEZE_VISION = True  # æ˜¾å­˜ä¼˜åŒ–ï¼šå†»ç»“è§†è§‰å¡”
USE_LORA = True

# --- 2. æ ¸å¿ƒä¼˜åŒ–ï¼šè‡ªå®šä¹‰ Data Collator (å¢å¼ºé²æ£’æ€§) ---
@dataclass
class QwenVideoDataCollator:
    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        # 1. æå–æ–‡æœ¬è¾“å…¥å’Œæ ‡ç­¾
        input_ids = [instance["input_ids"] for instance in instances]
        labels = [instance["labels"] for instance in instances]
        
        # 2. Pad æ–‡æœ¬éƒ¨åˆ† (batch_first=True)
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100 # Ignore index for loss
        )
        
        # 3. æˆªæ–­ (é˜²æ­¢å¼‚å¸¸æ•°æ®å¯¼è‡´OOM)
        input_ids = input_ids[:, :self.tokenizer.model_max_length]
        labels = labels[:, :self.tokenizer.model_max_length]
        
        # 4. æ„å»º Attention Mask
        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)

        batch = {
            "input_ids": input_ids,
            "labels": labels,
            "attention_mask": attention_mask,
        }

        # 5. [ä¼˜åŒ–] é²æ£’çš„è§†è§‰ç‰¹å¾æ”¶é›†
        # ä¸å†åªæ£€æŸ¥ instances[0]ï¼Œè€Œæ˜¯æ£€æŸ¥ batch ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•è§†è§‰ç‰¹å¾
        
        # å¤„ç†å›¾ç‰‡ (Pixel Values)
        if any("pixel_values" in inst for inst in instances):
            pixel_values = [inst["pixel_values"] for inst in instances if "pixel_values" in inst]
            image_grid_thw = [inst["image_grid_thw"] for inst in instances if "image_grid_thw" in inst]
            
            if len(pixel_values) > 0:
                batch["pixel_values"] = torch.cat(pixel_values, dim=0)
                batch["image_grid_thw"] = torch.cat(image_grid_thw, dim=0)

        # å¤„ç†è§†é¢‘ (Pixel Values Videos) - Qwen2.5-VL æ ¸å¿ƒ
        # å…¼å®¹ pixel_values_videos å’Œ video_pixel_values ä¸¤ç§å‘½å
        video_keys = ["pixel_values_videos", "video_pixel_values"]
        target_key = next((k for k in video_keys if any(k in inst for inst in instances)), None)

        if target_key:
            pv_videos = [inst[target_key] for inst in instances if target_key in inst]
            video_grid_thw = [inst["video_grid_thw"] for inst in instances if "video_grid_thw" in inst]
            
            if len(pv_videos) > 0:
                # å®˜æ–¹æ¨¡å‹ forward é»˜è®¤ä½¿ç”¨ 'pixel_values_videos'
                batch["pixel_values_videos"] = torch.cat(pv_videos, dim=0)
                batch["video_grid_thw"] = torch.cat(video_grid_thw, dim=0)

        return batch

# --- 3. æ•°æ®å¤„ç†å‡½æ•° ---
def process_func(example, processor, tokenizer):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": example["conversations"][0]["value"]},
                {"type": "text", "text": "Analyze the video. Is it Real or Generated?"}
            ]
        }
    ]
    
    # é¢„å¤„ç†è§†è§‰ä¿¡æ¯
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    # è¾“å…¥ Processor
    inputs = processor(
        text=[text], 
        images=image_inputs, 
        videos=video_inputs, 
        padding=False, # [é‡è¦] padding äº¤ç»™ Collatorï¼ŒèŠ‚çœå¤„ç†æ—¶é—´
        return_tensors="pt"
    )
    
    # å¤„ç† Label (Answer)
    response = example["conversations"][1]["value"]
    resp_tokens = tokenizer.encode(response, add_special_tokens=False)
    
    # æ„å»º Input IDs å’Œ Labels
    input_ids = inputs["input_ids"][0].tolist() + resp_tokens + [tokenizer.pad_token_id]
    labels = [-100] * len(inputs["input_ids"][0]) + resp_tokens + [tokenizer.pad_token_id]
    
    final_dict = {
        "input_ids": torch.tensor(input_ids, dtype=torch.long),
        "labels": torch.tensor(labels, dtype=torch.long),
    }
    
    # æå–è§†è§‰ç‰¹å¾å¹¶ç§»é™¤ batch ç»´åº¦ (processor è¾“å‡ºé€šå¸¸å¸¦ batch=1)
    if "pixel_values" in inputs:
        final_dict["pixel_values"] = inputs["pixel_values"] 
        final_dict["image_grid_thw"] = inputs["image_grid_thw"] # shape: (1, 3)
        
    if "pixel_values_videos" in inputs:
        final_dict["pixel_values_videos"] = inputs["pixel_values_videos"]
        final_dict["video_grid_thw"] = inputs["video_grid_thw"] # shape: (1, 3)
    elif "video_pixel_values" in inputs:
        final_dict["pixel_values_videos"] = inputs["video_pixel_values"]
        final_dict["video_grid_thw"] = inputs["video_grid_thw"]
            
    return final_dict

# --- 4. ä¸»ç¨‹åº ---
def train():
    # åˆå§‹åŒ– Processor
    processor = AutoProcessor.from_pretrained(
        MODEL_PATH, 
        min_pixels=256*28*28, 
        max_pixels=1280*28*28,
        padding_side="right"
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    config = AutoConfig.from_pretrained(MODEL_PATH)
    config._attn_implementation = "sdpa"
    # åŠ è½½æ¨¡å‹
    print("Loading model...")
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.bfloat16, 
        config=config,
        device_map=None 
    )

    # [ä¼˜åŒ–] å†»ç»“è§†è§‰å¡” (å‚è€ƒå®˜æ–¹é€»è¾‘)
    if FREEZE_VISION:
        print("â„ï¸ Freezing Vision Tower (saving ~30% memory)...")
        # Qwen2.5-VL çš„è§†è§‰éƒ¨åˆ†é€šå¸¸åœ¨ model.visual
        for param in model.visual.parameters():
            param.requires_grad = False
        # ç¡®ä¿ LLM éƒ¨åˆ†å‚ä¸è®­ç»ƒ

    # LoRA é…ç½®
    if USE_LORA:
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            r=64, 
            lora_alpha=16, 
            lora_dropout=0.05, 
            bias="none",
            modules_to_save=[] # ä¸ä¿å­˜ embedding/headï¼Œåªä¿å­˜ adapterï¼Œå‡å°æƒé‡ä½“ç§¯
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    # å‡†å¤‡æ•°æ®
    if not os.path.exists("train.json"): 
        raise FileNotFoundError("Run gen_cot_data.py first!")
    
    train_ds = Dataset.from_json("train.json")
    print(f"Loaded {len(train_ds)} samples from train.json")
    # åŒ…è£… process_func
    def _process(x): return process_func(x, processor, tokenizer)
    
    # é¢„å¤„ç†æ•°æ® (Map)
    print("Processing dataset...")
    train_dataset = train_ds.map(_process, remove_columns=train_ds.column_names)
    
    eval_dataset = None
    if os.path.exists("test.json"):
        eval_ds = Dataset.from_json("test.json").select(range(50)) # å°‘é‡éªŒè¯
        eval_dataset = eval_ds.map(_process, remove_columns=eval_ds.column_names)

    # SwanLab é…ç½®
    swanlab_callback = SwanLabCallback(
        project="Qwen2.5-VL-Video-Detection",
        experiment_name="Custom-Train-SFT",
        config={"freeze_vision": FREEZE_VISION, "max_length": MAX_LENGTH}
    )

    # è®­ç»ƒå‚æ•°
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1, # è§†é¢‘æ˜¾å­˜å¤§ï¼Œå»ºè®®ä¿æŒ1
        gradient_accumulation_steps=8, # ç´¯è®¡æ¢¯åº¦ï¼Œç­‰æ•ˆ batch=8
        num_train_epochs=3,
        learning_rate=1e-4,
        weight_decay=0.01,
        warmup_ratio=0.03,
        bf16=True, # å¿…é¡»å¼€å¯ bf16
        gradient_checkpointing=True, # å¿…é¡»å¼€å¯æ˜¾å­˜ä¼˜åŒ–
        dataloader_pin_memory=True,
        remove_unused_columns=False, # [é‡è¦] é˜²æ­¢ Collator éœ€è¦çš„è‡ªå®šä¹‰ key è¢«åˆ é™¤
        evaluation_strategy="steps" if eval_dataset else "no",
        eval_steps=50,
        save_steps=50,
        save_total_limit=2,
        logging_steps=5,
        report_to="none", # å…³é—­é»˜è®¤wandbï¼Œåªç”¨SwanLab
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=QwenVideoDataCollator(tokenizer),
        callbacks=[swanlab_callback],
    )

    print("ğŸš€ Starting training...")
    trainer.train()
    
    # ç»“æŸä¸ä¿å­˜
    swanlab.finish()
    trainer.save_model(f"{OUTPUT_DIR}/final")
    processor.save_pretrained(f"{OUTPUT_DIR}/final") # åŒæ—¶ä¿å­˜ processor é…ç½®
    print(f"Training finished. Model saved to {OUTPUT_DIR}/final")

if __name__ == "__main__":
    train()

# torchrun --nproc_per_node=auto --master_port=29500 train.py --deepspeed qwen-vl-finetune/scripts/zero3.json